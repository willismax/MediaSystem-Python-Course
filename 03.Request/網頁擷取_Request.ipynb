{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/willismax/MediaSystem-Python-Course/blob/main/03.Request/%E7%B6%B2%E9%A0%81%E6%93%B7%E5%8F%96_Request.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bxcSKyjgZtB"
      },
      "source": [
        "# 爬蟲-網頁資料擷取"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVxE3EyttF8k"
      },
      "source": [
        "- 擷取網頁用 [`requests`](https://docs.python-requests.org/en/latest/) 模組\n",
        "  - requests.get()\n",
        "  - requests.post()\n",
        "- 解析網頁用 [`BeautifulSoup`](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/) 模組\n",
        "  - soup.find()\n",
        "  - soup.find_all()\n",
        "  - soup.select()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cetpFdf0LVF"
      },
      "source": [
        "# Requests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYrMNwEeAvvx"
      },
      "source": [
        "\n",
        "#### 網頁請求的回應狀態碼（Status Code）\n",
        "當我們向網站發送請求時（比如點擊一個連結），網站會回傳一個「狀態碼」來告訴我們請求的結果。這就像是網站和我們之間的秘密語言：\n",
        "- `200 OK`：一切正常，你要的頁面在這裡！\n",
        "- `403 Forbidden`：不好意思，你不能進入這裡。\n",
        "- `404 Not Found`：沒有找到你要的頁面。\n",
        "\n",
        "#### 網頁內容的編碼（Encoding）\n",
        "有時網頁使用的文字編碼和我們的不同，這時我們需要調整編碼方式來正確讀取內容。比如：\n",
        "- `UTF-8`：最常見的編碼方式，支持多種語言。\n",
        "- `Big5`：繁體中文網站有時會使用的編碼。\n",
        "\n",
        "#### 請求的回應內容（Response）\n",
        "- `response.text`：這是網頁的 HTML 內容，也就是網頁的原始碼。\n",
        "- `response.json()`：如果回應的是 JSON 格式的資料，我們可以這樣將它轉換成 Python 能讀懂的格式（列表或字典）。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCbQZOMfFxBH"
      },
      "source": [
        "- 可從文件學習，搭配[requests官方文件quickstart服用](https://requests.readthedocs.io/en/latest/user/quickstart/)!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbc61gdWJ6n1"
      },
      "source": [
        "### 檢查連線資訊"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjvtq8j_aDVo"
      },
      "outputs": [],
      "source": [
        "import requests  # 引入 requests 模組\n",
        "\n",
        "url = \"https://api.github.com/events\"  # 設定要請求的網址\n",
        "r = requests.get(url)  # 向該網址發送 GET 請求\n",
        "\n",
        "r.json()  # 將回應的 JSON 內容轉換成 Python 能理解的格式\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-7VKFGVX6_b"
      },
      "outputs": [],
      "source": [
        "# 連線狀態\n",
        "r.status_code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYPGaXq4HiUr"
      },
      "outputs": [],
      "source": [
        "# 編碼\n",
        "r.encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQoFf4orT7u2"
      },
      "outputs": [],
      "source": [
        "# 內容\n",
        "r.content[:500]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9sIOw-fXyvB"
      },
      "outputs": [],
      "source": [
        "# 連線的錯誤訊息(正確連線則無)\n",
        "r.raise_for_status()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_eVG_e8ZLHB"
      },
      "outputs": [],
      "source": [
        "# cookies\n",
        "r.cookies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzr2Gz9CdPcZ"
      },
      "outputs": [],
      "source": [
        "# header (HTTP 標頭名稱不區分大小寫。)\n",
        "r.headers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KaznVFfKdTe9"
      },
      "outputs": [],
      "source": [
        "r.headers['Content-Type']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PO-DgdV4Avk"
      },
      "source": [
        "- Request讀取影音圖片檔案(二進位制)的方式"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIGVirc5JscG"
      },
      "source": [
        "### 快速認識`GET`、`POST`、`PUT`、`DELETE`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2aTkURA1cEE"
      },
      "outputs": [],
      "source": [
        "!curl -X GET \"https://api.github.com/events\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9ibgeIVs5ES"
      },
      "outputs": [],
      "source": [
        "# GET\n",
        "r = requests.get('https://httpbin.org/get')\n",
        "r.json()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ik3sWzjNOvvz"
      },
      "outputs": [],
      "source": [
        "# POST\n",
        "r = requests.post('https://httpbin.org/post', data={'key': 'value'})\n",
        "r.json()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1_2behXO5FB"
      },
      "outputs": [],
      "source": [
        "# PUT\n",
        "r = requests.put('https://httpbin.org/put', data={'key': 'value2'})\n",
        "r.json()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1OCPVTtUPA-L"
      },
      "outputs": [],
      "source": [
        "# DELETE\n",
        "r = requests.delete('https://httpbin.org/delete')\n",
        "r.json()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSPrfhvDONxz"
      },
      "outputs": [],
      "source": [
        "r.text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7P_uTgisJzKX"
      },
      "source": [
        "### `GET`，以及增加參數的方式"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nu5-xEIpQOq2"
      },
      "outputs": [],
      "source": [
        "!curl -X GET \"https://httpbin.org/get?k1=v1&k2=v2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-AWDMjlFnR1"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "payload = {'k1': 'v1', 'k2': 'v2'}\n",
        "r = requests.get('https://httpbin.org/get', params=payload)\n",
        "print(r.url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlVsV0zK2G7y"
      },
      "outputs": [],
      "source": [
        "# GET的參數會接在URL後面?\n",
        "import requests\n",
        "\n",
        "payload  = {\"api\":\"1\", \"map_action\":\"map\", \"zoom\":\"16\", \"query\":\"24.149660,120.684166\"}\n",
        "r = requests.get('https://www.google.com/maps/search/', params=payload )\n",
        "print(r.url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iixUXQhO4JEY"
      },
      "source": [
        "- `Request.get`大型檔案的方式，，`圖片、影音檔案、二進位制bin檔可用"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhHbSzOiJgbv"
      },
      "outputs": [],
      "source": [
        "# 來源: https://stackoverflow.com/questions/16694907/download-large-file-in-python-with-requests\n",
        "import requests\n",
        "\n",
        "def download_file(url):\n",
        "  \"\"\"下載檔案，檔名為url.split('/')[-1]\"\"\"\n",
        "  local_filename = url.split('/')[-1]\n",
        "  with requests.get(url, stream=True) as r:\n",
        "      r.raise_for_status()\n",
        "      with open(local_filename, 'wb') as f:\n",
        "          for chunk in r.iter_content(chunk_size=8192):\n",
        "              f.write(chunk)\n",
        "  return local_filename\n",
        "\n",
        "if __name__=='__main__':\n",
        "  download_file(\"https://api.github.com/events\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmhi4-huOaY-"
      },
      "source": [
        "### `POST`，帶有data的 POST 請求"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rcijz6DGbhCn"
      },
      "outputs": [],
      "source": [
        "!curl -X POST -d \"key1=value1&key2=value2\" \"https://httpbin.org/post\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPOmQawd56bK"
      },
      "source": [
        "- `payload_tuples`與`payload_dict`用法，以下兩者相同"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cO3IjjoyOpNe"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "payload_tuples = [('key1', 'value1'), ('key1', 'value2')]\n",
        "# payload_dict = {'key1': ['value1', 'value2']}\n",
        "# payload = {'key1': 'value1', 'key2': 'value2'}\n",
        "\n",
        "requests.post('https://httpbin.org/post', data=payload_tuples).json()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1uIbe94Pjmb"
      },
      "source": [
        "-  `post(url, data=None, json=None, **kwargs)`，以下示範參數放dict轉json或直接json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snYNC7nPPfwU"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "url = 'https://httpbin.org/post'\n",
        "payload = {'some': 'data'}\n",
        "\n",
        "# 以下兩種相同\n",
        "r = requests.post(url, data=json.dumps(payload))\n",
        "r = requests.post(url, json=payload)\n",
        "\n",
        "r.json()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95RouNSnoAkj"
      },
      "source": [
        "- 加入`Cookies`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdbxuU1G2dWK"
      },
      "outputs": [],
      "source": [
        "!curl --cookie \"my_cookie=22222\" https://httpbin.org/cookies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JyzrAU8poBV"
      },
      "outputs": [],
      "source": [
        "url = 'https://httpbin.org/cookies'\n",
        "cookies = dict(cookies_are='working')\n",
        "r = requests.get(url, cookies=cookies)\n",
        "r.json()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3fahI50tvfq"
      },
      "source": [
        "- Timeouts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WpIhTGGhtwly"
      },
      "outputs": [],
      "source": [
        "requests.get('https://github.com/', timeout=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iodaXDy83xtV"
      },
      "source": [
        "## `GET` example網頁為例"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKprYoPf31yH"
      },
      "source": [
        "\n",
        "- 先觀察目標網頁: http://www.example.com/\n",
        "- 以`requests.get`抓取網頁原始碼，並輸出結果\n",
        "- 這個階段有抓到網頁就大功告成了!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GFnb4ACf3ad"
      },
      "outputs": [],
      "source": [
        "import requests  # 引入 requests 模組\n",
        "\n",
        "url = \"https://api.github.com/events\"  # 設定要請求的網址\n",
        "r = requests.get(url)  # 向該網址發送 GET 請求\n",
        "\n",
        "r.json()  # 將回應的 JSON 內容轉換成 Python 能理解的格式\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0YRPSyeSPp6"
      },
      "outputs": [],
      "source": [
        "dir(requests)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36Qif3AdSexz"
      },
      "outputs": [],
      "source": [
        "r.status_code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rmZASQ0H5Li"
      },
      "outputs": [],
      "source": [
        "r.encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bIWlzMJd_PJ"
      },
      "source": [
        "## `POST` [台灣高鐵訂票](https://www.thsrc.com.tw/ArticleContent/a3b630bb-1066-4352-a1ef-58c7b4e8ef7c)為例"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4mHnNWaA_vf"
      },
      "source": [
        "![image.png](https://hackmd.io/_uploads/SyFC06yma.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srdMH3bPZ5b_"
      },
      "source": [
        "![image](https://hackmd.io/_uploads/SyCjZ7VV6.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dCp9XUzeHjV"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "url= 'https://www.thsrc.com.tw/TimeTable/Search'\n",
        "\n",
        "data={\n",
        "    'SearchType': 'S',\n",
        "    'Lang': 'TW',\n",
        "    'StartStation': 'NanGang',\n",
        "    'EndStation': 'ZuoYing',\n",
        "    'OutWardSearchDate': '2025/05/22',\n",
        "    'OutWardSearchTime': '16:00',\n",
        "    'ReturnSearchDate': '2014/05/22',\n",
        "    'ReturnSearchTime': '16:00',\n",
        "    'DiscountType': None\n",
        "}\n",
        "\n",
        "res = requests.post(url, data=data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yHWIPRrGz-U"
      },
      "outputs": [],
      "source": [
        "res.json()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdEKMmBlwJHD"
      },
      "outputs": [],
      "source": [
        "res.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-4XiprkCB70"
      },
      "outputs": [],
      "source": [
        "res.headers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLKTxQUXxryP"
      },
      "outputs": [],
      "source": [
        "r = res.json()\n",
        "r['data']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S86BgvqY0Src"
      },
      "source": [
        "# BeautifulSoup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxl9C3YOIMGq"
      },
      "source": [
        "## 以Beautiful Soup讀取並解析HTML\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5J2uIUKkUIU"
      },
      "source": [
        "- [文件](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/)\n",
        "- Beautiful Soup是HTML解析器，將網頁解析為 `bs4.BeautifulSoup` 物件。\n",
        "- `bs4.BeautifulSoup` 物件是個結構樹(DOM)，依結構與各種方法搜尋目標。\n",
        "```\n",
        "!pip3 install beautifulsoup4\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bMZLxcR0vso"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "html_doc=\"\"\"<html><head><title>The Dormouse's story</title></head>\n",
        "<body>\n",
        "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
        "\n",
        "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
        "<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\n",
        "<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
        "<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\n",
        "and they lived at the bottom of a well.</p>\n",
        "\n",
        "<p class=\"story\">...</p>\"\"\"\n",
        "\n",
        "\n",
        "soup = BeautifulSoup(html_doc, 'html.parser')\n",
        "print(soup.prettify())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skguKwRaZfqY"
      },
      "source": [
        "下表列出了主要的解析器，以及它們的優缺點：\n",
        "\n",
        "解析器|使用方法|優勢|\t劣勢|\n",
        "-|-|-|-\n",
        "html.parser|\tBeautifulSoup(markup,\"html.parser\")\t|Python的內建標準庫、執行速度適中、文檔容錯能力強|Python 2.7.3及3.2.2之前的版本中文檔容錯能力差\n",
        "lxml HTML 解析器|\tBeautifulSoup(markup, \"lxml\")\t|速度快、文檔容錯能力強(通常用這個)|需要安装C语言库\n",
        "xml XML 解析器|BeautifulSoup(markup, \"xml\")|速度快、唯一支持XML的解析器|需要安装C语言库\n",
        "html5lib\t|BeautifulSoup(markup, \"html5lib\")\t|最好的容錯性、以瀏覽器的方式解析文檔、生成HTML5格式的文檔|速度慢、不依賴外部擴展\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qP1zGwhwiS7K"
      },
      "source": [
        "### 簡易解析文件\n",
        "- 用`.`的方式存取物件結構，快速但容易出錯\n",
        "- 用`find()`、`find_all()`、`select()`方法較嚴謹"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqZfNTrEiAyH"
      },
      "outputs": [],
      "source": [
        "soup.title"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LIP9oOJiC-a"
      },
      "outputs": [],
      "source": [
        "soup.title.name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aesLKUl6iEu4"
      },
      "outputs": [],
      "source": [
        "soup.title.string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "az2qGPQmmpdN"
      },
      "outputs": [],
      "source": [
        "soup.title.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUgVbrQ9iHzD"
      },
      "outputs": [],
      "source": [
        "soup.title.parent.name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4aNxYa5fiprU"
      },
      "outputs": [],
      "source": [
        "soup.p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XaYmOcueirnZ"
      },
      "outputs": [],
      "source": [
        "soup.p['class']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RekX2eFfLPUC"
      },
      "outputs": [],
      "source": [
        "soup.p.get(\"class\") #推薦使用`.get()`取得屬性"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6iLQ7GRiwrB"
      },
      "outputs": [],
      "source": [
        "soup.a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4sybp4xi1ZC"
      },
      "outputs": [],
      "source": [
        "soup.a[\"href\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXfVft0ELfJP"
      },
      "outputs": [],
      "source": [
        "soup.a.get('href')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rx1iadEvjfaJ"
      },
      "outputs": [],
      "source": [
        "print(type(soup.title))\n",
        "print(type(soup.p))\n",
        "print(type(soup.a))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSBaLK7W3vga"
      },
      "outputs": [],
      "source": [
        "for link in soup.find_all('a'):\n",
        "    print(link.get('href'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDbLOG8A3yff"
      },
      "outputs": [],
      "source": [
        "[ i.get(\"href\") for i in soup.find_all('a') ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bgtv3fFn6AD2"
      },
      "source": [
        "### `soup.find()`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQPcgAtPkoJ3"
      },
      "source": [
        "-  回傳第一個被tag包圍的區塊\n",
        "- 傳入的引數第一個通常是 tag 名稱，第二個引數若未指明屬性就代表 class 名稱，也可以直接使用 id 等屬性去定位區塊。定位到區塊後，可以取出其屬性與包含的字串值\n",
        "\n",
        "  ```python\n",
        "  soup.find(name=None,    # 第一個tag name\n",
        "      attrs={},      # {”屬性名”=“屬性值”}\n",
        "      recursive=True,  # 迴圈搜尋開啟\n",
        "      text=None,    # 查找內文\n",
        "      **kwargs)\n",
        "  ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-F3nFNZF6XUA"
      },
      "outputs": [],
      "source": [
        "help(soup.find())\n",
        "#soup.find(name=None, attrs={}, recursive=True, text=None, **kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFyV0EaQ50KG"
      },
      "outputs": [],
      "source": [
        "print(soup.find('p'))\n",
        "print(soup.find(\"a\"))\n",
        "\n",
        "#取<a>內容</a>\n",
        "print(soup.find(\"a\").string)\n",
        "print(soup.find(\"a\").text)\n",
        "\n",
        "#取<title>標題</title>，\n",
        "print(soup.title.string)\n",
        "print(soup.title.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGUUCPFfm_ZV"
      },
      "source": [
        "### `soup.find().get(屬性)`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGKFDMlZDCU1"
      },
      "source": [
        "- 取出節點屬性的較好方法`.get(\"屬性\")`\n",
        "  - 使用`get()`如無此屬性，回傳結果為none。\n",
        "  - 如果不用`get()`也可以擷取屬性，但不存在時會出現錯誤，有礙後續爬蟲執行。\n",
        "  - 其他詳細用法可參考 [BeautifulSoup的官方文件](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ck6CqC5upT4t"
      },
      "outputs": [],
      "source": [
        "# 找不到屬性就出錯! #id, class, href, src\n",
        "soup.find(\"p\")['style']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XkEDEZTEpiOF"
      },
      "outputs": [],
      "source": [
        "# 找不到屬性回傳None\n",
        "print(soup.find('p').get('style'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lX0Q_MEdepVA"
      },
      "source": [
        "### `soup.find_all()`\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjT29um4ndQ9"
      },
      "source": [
        "- 我全都要，回傳結果為`bs4.element.ResultSet`物件\n",
        "  ```python\n",
        "  soup.find_all(name=None,     #第一個tag name\n",
        "         attrs={},      #{”屬性名”=“屬性值”}\n",
        "         text=None,     #查找內文\n",
        "         limit=None,     #限制搜尋數量\n",
        "         **kwargs)\n",
        "  ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYarGEZvnoow"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "res = requests.get('https://www.python.org/')\n",
        "soup = BeautifulSoup(res.text, \"lxml\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7R0n1YHnnsE"
      },
      "outputs": [],
      "source": [
        "p_tags = soup.find_all(\"p\")\n",
        "p_tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1bYlMczFy5z"
      },
      "outputs": [],
      "source": [
        "type(p_tags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRgwqCBTrF9o"
      },
      "outputs": [],
      "source": [
        "# 找出所有內容等於的文字\n",
        "print(soup.find_all(text=\"Latest News\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsQsDD2SoNMg"
      },
      "source": [
        "- `bs4.element.ResultSet`物件內容以for迴圈取出"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2WOX3w8dKAn"
      },
      "outputs": [],
      "source": [
        "for tag in p_tags:\n",
        "  print(tag)\n",
        "  print(type(tag)) # 取出一層，內層是`bs4.element.Tag`物件"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-qbhw_1oi6S"
      },
      "outputs": [],
      "source": [
        "for tag in p_tags:\n",
        "  print(tag.text)\n",
        "  print(type(tag.text)) # 已解析內文，為文字str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNMNUGqIBNRV"
      },
      "outputs": [],
      "source": [
        "# 取出節點屬性\n",
        "\n",
        "a_tags = soup.find_all(\"a\")\n",
        "for tag in a_tags:\n",
        "  print(tag.get('href'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhVDY1zNpcmy"
      },
      "source": [
        "- `soup.find_all()`以list`[]`同時搜尋多種標籤"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFKxz_XoBe3P"
      },
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "\n",
        "tags = soup.find_all([\"a\", \"b\", \"p\"]) # 搜尋所有超連結與粗體字\n",
        "pprint(tags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QdWbrjToCZJe"
      },
      "outputs": [],
      "source": [
        "tags = soup.find_all([\"a\", \"p\"], limit=2) # 限制搜尋結果數量limit\n",
        "pprint(tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aLRQSw9HuZ0"
      },
      "source": [
        "### `soup.select()`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaMkgMgTtZVR"
      },
      "source": [
        "- 用CSS Seletor選擇器，結果回傳為list\n",
        "- list裡面如果還是標籤形式，這些標籤還是`bs`物件，要解出來才能接著python操作\n",
        "\n",
        "```python\n",
        "select(selector, _candidate_generator=None, limit=None)\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gs024RN3vl9c"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "res = requests.get('http://www.example.com/')\n",
        "soup = BeautifulSoup(res.text, \"lxml\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYb6livXvo0c"
      },
      "outputs": [],
      "source": [
        "select_a = soup.select(\"a\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsEnAh36wFar"
      },
      "outputs": [],
      "source": [
        "print(type(select_a))\n",
        "select_a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ucczof1vyvi"
      },
      "outputs": [],
      "source": [
        "print(type(select_a[0]))\n",
        "print(select_a[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EcAnjLQ2v7Xx"
      },
      "outputs": [],
      "source": [
        "#解析內文\n",
        "print(type(select_a[0]).text)\n",
        "\n",
        "select_a[0].text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8m7gTGhwNjs"
      },
      "outputs": [],
      "source": [
        "select_href1 = soup.select('[href]')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9YUGKinNwpVu"
      },
      "outputs": [],
      "source": [
        "print(type(select_href1))\n",
        "\n",
        "print(select_href1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzxFg2jlwrNo"
      },
      "outputs": [],
      "source": [
        "print(type(select_href1[0]))\n",
        "print(select_href1[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hjlvmti9wzXr"
      },
      "outputs": [],
      "source": [
        "#配合`.get(屬性)`來解析屬性\n",
        "print(type(select_href1[0].get('href')))\n",
        "print(select_href1[0].get('href'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2Y1wQN7IQiU"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "res = requests.get('http://python.org/')\n",
        "soup = BeautifulSoup(res.text, \"lxml\")\n",
        "a1=soup.select(\"#touchnav-wrapper > header > div > h1 > a > img\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdW1cAuEKdcb"
      },
      "outputs": [],
      "source": [
        "a1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhlAE_M0OvP-"
      },
      "outputs": [],
      "source": [
        "a1[0].get(\"src\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbkO2Q-K9QCi"
      },
      "source": [
        "## 結合正規表達式regular expression進行搜尋\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNSsgL9jx250"
      },
      "source": [
        "- 正規表達式對於精準抓取網頁的各種標籤及內文非常有幫助，解決了許多Xpath與CSS selector無法精確擷取的問題，有必要好好理解。\n",
        "- 擷取的文句段落可以使用[regex101.com](https://regex101.com/)測試。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFqB9BQol4IN"
      },
      "source": [
        "\n",
        "|符號|意義|範例|符合字串範例\n",
        "|-|-|-|-\n",
        "|`*`|`*`之前的字元、表達式或`[]`字元集合，出現為0或1個以上|`a*b*`|aaaa、aaabb、bbbb\n",
        "|`+`|`+`之前的字元、表達式或`[]`字元集合，出現為1或1個以上|`a+b+`|aaab、aabbb、abbb\n",
        "|`?`|`+`之前的字元、表達式或`[]`字元集合，出現為0或1次|`a?b?`|ab、b\n",
        "|`[]`|`[]`內的任一字元挑一個|`[A-Z]*`|ALLPE、CAP、QWER\n",
        "|`()`|`()`群組，群組運算優先處理|`(a*b)*`|aabaab、abaab、ababab\n",
        "|`{m,n}`|符合在`{m,n}`前一個字元、表達式或`[]`集合，出現m到n次(包含m與n|`a{2,3}b{2,3}`|aabbb、aaabbb、aabb\n",
        "|`[^]`|符合任一個不再`[]`的字元|`[^A-Z]*`|apple、banana、cat\n",
        "|`\\|`|符合被`\\|`隔開的前後任一字元、字串或表達式|`b(a\\|i\\|e)d`|bad、bid、bed\n",
        "|`.`|符合任一字元(含符號、數字、空格等)|`b.d`|bsd、bid、bed\n",
        "|`^`|`^`之後的第1個字元為開頭的字串|`^a`|apple、afk\n",
        "|`$`|`$`之前的末1個字元為結尾，否則會`.*`|`[A-Z]*[a-z]*$`|Aab、zzz\n",
        "|`\\d`|所有數字|`\\d`|455、5566\n",
        "|`\\w`|所有文字字元|`\\w`|123ABC、C8763\n",
        "|`\\s`|所有非無的字元與操作|`\\s`|`Tab, Space, Escape, …`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhyq1jQH_Hzm"
      },
      "source": [
        "#### Python的re模組\n",
        "- 可至[regex101](https://regex101.com/)嘗試\n",
        "- 為了避免與字串中的跳脫字元產生混淆，定義正規表達式樣式建議使用原始字串(raw string)，也就是在字串前加r''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMcBImeQIiDh"
      },
      "source": [
        "##### 參考寫法\n",
        "```python\n",
        "import re\n",
        "\n",
        "# 找出所有內容等於 python_crawler 的文字\n",
        "pattern = \"我寫好的 regular expression\"\n",
        "string = \"我想要找的字串\"\n",
        "re.findall(pattern, string)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZDDwMTBL2Nh"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "pattern = \"我\"\n",
        "string = \"我想要找的字串我我\"\n",
        "re.findall(pattern, string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYwDH1z1Yy5j"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "pattern = \"^[a-zA-Z0-9\\._-]+@[a-zA-Z0-9\\._-]+$\"\n",
        "string = \"willismax.com@gmail.com\"\n",
        "re.findall(pattern, string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdZYbCHWc7eM"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import re\n",
        "\n",
        "res = requests.get('http://python.org/')\n",
        "\n",
        "pattern = r'h[1-6]' #標題h1-h6\n",
        "string = res.text\n",
        "re.findall(pattern, string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5cBsSTM4d1oq"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "res = requests.get('http://python.org/')\n",
        "\n",
        "pattern = r'\"\\S*.png\"' # .jpg或.png結尾\n",
        "string = res.text\n",
        "re.findall(pattern, string)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_InTBJdlqQNE"
      },
      "source": [
        "## 網頁擷取實例\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrUdffk-pK7M"
      },
      "source": [
        "### 以PPT 為例\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qF4cq73HpMg1"
      },
      "source": [
        "- 這邊開始要示範使用Chrome開發者工具進行搜尋\n",
        "- 先觀察目標網頁: https://www.ptt.cc/bbs/StupidClown/index.html\n",
        "- 使用Chrome瀏覽器，以滑鼠右鍵選擇「檢查」，快捷鍵在windows環境為ctrl+Shift+I或F12\n",
        "\n",
        "- 另外如果要用別人寫好的，參閱https://dotblogs.com.tw/codinghouse/2018/10/22/pttcrawler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sFqQzBvBxIZ"
      },
      "source": [
        "```\n",
        "//*[@id=\"main-container\"]/div[2]/div[2]/div[2]/a\n",
        "#main-container > div.r-list-container.action-bar-margin.bbs-screen > div:nth-child(2) > div.title > a\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMdTi4BBfd-C"
      },
      "source": [
        "![](https://i.imgur.com/K55v4SH.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiNSwFzUhMb9"
      },
      "source": [
        "- 文章列表可以觀察到推文數、文章標題、作者、日期及文章連結\n",
        "- 我們先觀察他的樹狀結構，對應的標籤與屬性\n",
        "- 以COPY XPath紀錄\n",
        "\n",
        "|名稱|selector|\n",
        "-|-\n",
        "標題|`//*[@id=\"main-container\"]/div[2]/div[4]/div[2]/a`\n",
        "連結|`//*[@id=\"main-container\"]/div[2]/div[4]/div[2]/a`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFIcyv2v6hRg"
      },
      "outputs": [],
      "source": [
        "#目標網址https://www.ptt.cc/bbs/StupidClown/index.html\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "res = requests.get('https://www.ptt.cc/bbs/StupidClown/index.html')\n",
        "soup = BeautifulSoup(res.text ,\"html.parser\")\n",
        "print(res.text[:500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSjTlja9pqx_"
      },
      "outputs": [],
      "source": [
        "print(soup.prettify())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oW-IKihpv0HR"
      },
      "source": [
        "- 有抓到網頁，接下來如果簡單針對連結、標題的話，觀察都在div標籤的class='title'裡"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NorSJDRXBok"
      },
      "outputs": [],
      "source": [
        "# #main-container > div.r-list-container.action-bar-margin.bbs-screen > div:nth-child(5) > div.title > a\n",
        "\n",
        "results = soup.select(\"div.title > a\")\n",
        "print(results)\n",
        "print(type(results))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRrphsEiq8YP"
      },
      "outputs": [],
      "source": [
        "article_href = soup.select(\"div.title a\")\n",
        "article_href"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKnxpxaorHjr"
      },
      "outputs": [],
      "source": [
        "# 逐一取出標題、合併超連結\n",
        "for a in article_href:\n",
        "  print(f'{a.text}')\n",
        "  print(f'href: https://www.ptt.cc{a.get(\"href\")}')\n",
        "\n",
        "  #打開連結內的網頁並另存\n",
        "  content_url = f'https://www.ptt.cc{a.get(\"href\")}'\n",
        "  r = requests.get(content_url)\n",
        "  with open (f'{a.text}.html', 'w+') as f:\n",
        "    f.write(r.text)\n",
        "    print('saved')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkRd8kgpuIQh"
      },
      "outputs": [],
      "source": [
        "%ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDF2XVSvNs_4"
      },
      "source": [
        "- 更多可參考[爬蟲教學 CrawlerTutorial](https://github.com/leVirve/CrawlerTutorial)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdsBCb4BkOkK"
      },
      "source": [
        "### 以wiki亞洲國家資訊為例"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rINKbz4-kXl8"
      },
      "source": [
        "- 參考來源[Web Scraping Wikipedia Tables using BeautifulSoup and Python](https://medium.com/analytics-vidhya/web-scraping-wiki-tables-using-beautifulsoup-and-python-6b9ea26d8722)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwJK3gS2kpmJ"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = \"https://en.wikipedia.org/wiki/List_of_Asian_countries_by_area\"  # 設定要請求的網址\n",
        "res = requests.get(url)  # 向該網址發送 GET 請求\n",
        "soup = BeautifulSoup(res.text ,\"html.parser\")\n",
        "\n",
        "res.text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysX4Mq1JlKTT"
      },
      "source": [
        "![](https://miro.medium.com/max/740/1*NyaaGqqHnemKSWu8DQqUHQ.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELYuAc4plPCN"
      },
      "outputs": [],
      "source": [
        "table_href = soup.select(\"table.wikitable.sortable\")\n",
        "table_href"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7n_vzVClthW"
      },
      "outputs": [],
      "source": [
        "country = [\n",
        "        link.get('title')\n",
        "        for link in table_href\n",
        "        if link.get('title') != None\n",
        "        ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRTNH6y_JAOy"
      },
      "outputs": [],
      "source": [
        "country"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QaTp8NcmdGZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame()\n",
        "df['Country'] = country\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Hdt3qwunW_g"
      },
      "outputs": [],
      "source": [
        "df = df.sort_values(by=\"Country\").reset_index(drop = True)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnv7fzVMaPCz"
      },
      "source": [
        "# 練習"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ta5rh-Up1vQe"
      },
      "source": [
        "##  練習1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E76LBd6BaSRE"
      },
      "source": [
        "- 試著看懂並執行、拆解以下程式"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime\n",
        "import json\n",
        "\n",
        "# 定義 PTT 的 URL\n",
        "PTT_URL = 'https://www.ptt.cc'\n",
        "\n",
        "def get_web_page(url):\n",
        "    \"\"\"\n",
        "    透過 URL 獲取網頁內容。\n",
        "    使用 requests 庫進行 HTTP 請求，並處理可能的異常。\n",
        "    \"\"\"\n",
        "    try:\n",
        "        resp = requests.get(url, cookies={'over18': '1'})  # 設定 cookies 以通過年齡限制\n",
        "        resp.raise_for_status()  # 檢查請求是否成功，若不成功則拋出異常\n",
        "        return resp.text\n",
        "    except requests.RequestException as e:\n",
        "        print(f'Error fetching {url}: {e}')\n",
        "        return None\n",
        "\n",
        "def parse_articles(dom, date):\n",
        "    \"\"\"\n",
        "    解析 HTML 文檔，提取符合指定日期的文章資訊。\n",
        "    \"\"\"\n",
        "    soup = BeautifulSoup(dom, 'html5lib')\n",
        "    articles = []\n",
        "    for d in soup.find_all('div', class_='r-ent'):\n",
        "        post_date = d.find('div', class_='date').text.strip()\n",
        "        if post_date == date:\n",
        "            push_count = get_push_count(d)\n",
        "            if link := d.find('a'):\n",
        "                articles.append({\n",
        "                    'title': link.text,\n",
        "                    'href': PTT_URL + link['href'],\n",
        "                    'push_count': push_count\n",
        "                })\n",
        "    prev_url = get_prev_page_url(soup)\n",
        "    return articles, prev_url\n",
        "\n",
        "def get_push_count(div):\n",
        "    \"\"\"\n",
        "    從文章區塊解析推文數。\n",
        "    推文數可能是數字、'爆' 表示非常熱門，或以 'X' 開頭表示負面推文。\n",
        "    \"\"\"\n",
        "    push_str = div.find('div', class_='nrec').text\n",
        "    try:\n",
        "        return int(push_str) if push_str else 0\n",
        "    except ValueError:\n",
        "        return 99 if push_str == '爆' else -10\n",
        "\n",
        "def get_prev_page_url(soup):\n",
        "    \"\"\"\n",
        "    從導航區塊提取上一頁的 URL。\n",
        "    \"\"\"\n",
        "    paging_div = soup.find('div', 'btn-group btn-group-paging')\n",
        "    return paging_div.find_all('a')[1]['href']\n",
        "\n",
        "def fetch_today_articles(url):\n",
        "    \"\"\"\n",
        "    獲取今日的文章列表。\n",
        "    從起始 URL 開始，遞迴獲取每頁的文章直到找不到符合日期的文章為止。\n",
        "    \"\"\"\n",
        "    articles = []\n",
        "    date_today = datetime.now().strftime('%m/%d').lstrip('0')  # 獲取今天的日期，並格式化\n",
        "    while True:\n",
        "        page = get_web_page(url)\n",
        "        if not page:\n",
        "            break\n",
        "        current_articles, prev_url = parse_articles(page, date_today)\n",
        "        if not current_articles:\n",
        "            break\n",
        "        articles.extend(current_articles)\n",
        "        url = PTT_URL + prev_url\n",
        "    return articles\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    主函式：獲取今天在 PTT 八卦版的文章並輸出熱門文章。\n",
        "    \"\"\"\n",
        "    start_url = PTT_URL + '/bbs/Gossiping/index.html'\n",
        "    articles = fetch_today_articles(start_url)\n",
        "\n",
        "    print(f'今天有{len(articles)}篇文章')\n",
        "    threshold = 50  # 設定熱門文章的推文閾值\n",
        "    print(f'熱門文章(>{threshold}推):')\n",
        "    for article in filter(lambda a: a['push_count'] > threshold, articles):\n",
        "        print(article)\n",
        "\n",
        "    # 將結果存儲為 JSON 檔案\n",
        "    with open('gossiping.json', 'w', encoding='utf-8') as f:\n",
        "        json.dump(articles, f, indent=2, sort_keys=True, ensure_ascii=False)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "Cks5YPEb-60T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 介面實作範例"
      ],
      "metadata": {
        "id": "yCOBHirgbejT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gradio"
      ],
      "metadata": {
        "id": "_D6EhEysaWUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime\n",
        "import json\n",
        "import gradio as gr\n",
        "\n",
        "PTT_URL = 'https://www.ptt.cc'\n",
        "\n",
        "def get_web_page(url):\n",
        "    try:\n",
        "        resp = requests.get(url, cookies={'over18': '1'})\n",
        "        resp.raise_for_status()\n",
        "        return resp.text\n",
        "    except requests.RequestException as e:\n",
        "        print(f'Error fetching {url}: {e}')\n",
        "        return None\n",
        "\n",
        "def parse_articles(dom, date):\n",
        "    soup = BeautifulSoup(dom, 'html5lib')\n",
        "    articles = []\n",
        "    for d in soup.find_all('div', class_='r-ent'):\n",
        "        post_date = d.find('div', class_='date').text.strip()\n",
        "        if post_date == date:\n",
        "            push_count = get_push_count(d)\n",
        "            if link := d.find('a'):\n",
        "                articles.append({\n",
        "                    'title': link.text,\n",
        "                    'href': PTT_URL + link['href'],\n",
        "                    'push_count': push_count\n",
        "                })\n",
        "    prev_url = get_prev_page_url(soup)\n",
        "    return articles, prev_url\n",
        "\n",
        "def get_push_count(div):\n",
        "    push_str = div.find('div', class_='nrec').text\n",
        "    try:\n",
        "        return int(push_str) if push_str else 0\n",
        "    except ValueError:\n",
        "        return 99 if push_str == '爆' else -10\n",
        "\n",
        "def get_prev_page_url(soup):\n",
        "    paging_div = soup.find('div', 'btn-group btn-group-paging')\n",
        "    return paging_div.find_all('a')[1]['href']\n",
        "\n",
        "def fetch_today_articles(board):\n",
        "    url = f\"{PTT_URL}/bbs/{board}/index.html\"\n",
        "    articles = []\n",
        "    date_today = datetime.now().strftime('%m/%d').lstrip('0')\n",
        "    while True:\n",
        "        page = get_web_page(url)\n",
        "        if not page:\n",
        "            break\n",
        "        current_articles, prev_url = parse_articles(page, date_today)\n",
        "        if not current_articles:\n",
        "            break\n",
        "        articles.extend(current_articles)\n",
        "        url = PTT_URL + prev_url\n",
        "    return [a for a in articles if a['push_count'] > 50]\n",
        "\n",
        "def display_articles(board):\n",
        "    articles = fetch_today_articles(board)\n",
        "    return json.dumps(articles, indent=2, ensure_ascii=False)\n",
        "\n",
        "interface = gr.Interface(\n",
        "    fn=display_articles,\n",
        "    inputs=gr.Dropdown([\"Tech_Job\", \"Gossiping\", \"NBA\", \"C_Chat\", \"Stock\"], label=\"選擇看板\"),\n",
        "    outputs=\"json\",\n",
        "    title=\"PTT 熱門文章列表\",\n",
        "    description=\"選擇 PTT 看板，顯示推文數大於 50 的文章列表\"\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    interface.launch()\n"
      ],
      "metadata": {
        "id": "qSiZaWOYaAdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgZmPsh9zZlN"
      },
      "source": [
        "## 練習2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3GT1nsfzhEy"
      },
      "source": [
        "- 擷取並parse「批批踢JOKE版的一篇文章」\n",
        "- 請依下列步驟練習：\n",
        "    - 以GET方法將網頁https://www.ptt.cc/bbs/joke/M.1571755669.A.663.html 原始碼讀入\n",
        "    - 依照上述步驟parse出推文內容及推文者\n",
        "    - 透過for迴圈，整齊印出"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "0bxcSKyjgZtB",
        "1cetpFdf0LVF",
        "dYrMNwEeAvvx",
        "sbc61gdWJ6n1",
        "yIGVirc5JscG",
        "7P_uTgisJzKX",
        "nmhi4-huOaY-",
        "iodaXDy83xtV",
        "7bIWlzMJd_PJ",
        "S86BgvqY0Src",
        "uxl9C3YOIMGq",
        "qP1zGwhwiS7K",
        "Bgtv3fFn6AD2",
        "kGUUCPFfm_ZV",
        "lX0Q_MEdepVA",
        "4aLRQSw9HuZ0",
        "PbkO2Q-K9QCi",
        "bhyq1jQH_Hzm",
        "tMcBImeQIiDh",
        "RrUdffk-pK7M",
        "sdsBCb4BkOkK",
        "vnv7fzVMaPCz",
        "Ta5rh-Up1vQe",
        "wyXAXDIN1MBI",
        "rgZmPsh9zZlN"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}